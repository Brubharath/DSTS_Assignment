{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Technology and Systems\n",
    "\n",
    "### Master of Data Science\n",
    "### U3241627 - Bharath Shivakumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Predicting Airplane Delays\n",
    "\n",
    "The goals of this notebook are:\n",
    "- Process and create a dataset from downloaded ZIP files\n",
    "- Exploratory data analysis (EDA)\n",
    "- Establish a baseline model and improve it\n",
    "\n",
    "## Introduction to business scenario\n",
    "You work for a travel booking website that is working to improve the customer experience for flights that were delayed. The company wants to create a feature to let customers know if the flight will be delayed due to weather when the customers are booking the flight to or from the busiest airports for domestic travel in the US. \n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to identify whether the flight will be delayed due to weather. You have been given access to the a dataset of on-time performance of domestic flights operated by large air carriers. You can use this data to train a machine learning model to predict if the flight is going to be delayed for the busiest airports.\n",
    "\n",
    "### Dataset\n",
    "The provided dataset contains scheduled and actual departure and arrival times reported by certified US air carriers that account for at least 1 percent of domestic scheduled passenger revenues. The data was collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS). The dataset contains date, time, origin, destination, airline, distance, and delay status of flights for flights between 2014 and 2018.\n",
    "The data are in 60 compressed files, where each file contains a CSV for the flight details in a month for the five years (from 2014 - 2018). The data can be downloaded from this link: [https://ucstaff-my.sharepoint.com/:f:/g/personal/ibrahim_radwan_canberra_edu_au/Er0nVreXmihEmtMz5qC5kVIB81-ugSusExPYdcyQTglfLg?e=bNO312]. Please download the data files and place them on a relative path. Dataset(s) used in this assignment were compiled by the Office of Airline Information, Bureau of Transportation Statistics (BTS), Airline On-Time Performance Data, available with the following link: [https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the environment \n",
    "\n",
    "Use one of the labs which we have practised on with the Amazon Sagemakers where you perform the following steps:\n",
    "1. Start a lab.\n",
    "2. Create a notebook instance and name it \"oncloudproject\".\n",
    "3. Increase the used memory to 25 GB from the additional configurations.\n",
    "4. Open Jupyter Lab and upload this notebook into it.\n",
    "5. Upload the two combined CVS files (combined_csv_v1.csv and combined_csv_v2.csv), which you created in Part A of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build and evaluate simple models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use linear learner estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey and to comments on the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the data\n",
    "data = pd.read_csv('combined_csv_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check the shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check to see if there are any NA values in the data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do see some negligible amount of NA values, let's drop them\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check the NA values again\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and testing sets (70% - 15% - 15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the data in train, test and validation accordingly.\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3, random_state= 42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us understand these 3 variables\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We save the data\n",
    "train_data.to_csv(\"train_data_v1.csv\", index = False, header = False)\n",
    "validation_data.to_csv(\"validation_data_v1.csv\", index = False, header = False)\n",
    "test_data_val = test_data.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the sagemaker session and the region\n",
    "sess_sage = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "##We create a default bucket\n",
    "bucket = sess_sage.default_bucket()\n",
    "prefix = \"sagemaker/oncloud\"\n",
    "\n",
    "#We define the flies that are going to be uploaded\n",
    "train_file='train_data_v1.csv'\n",
    "test_file='test_data_v1.csv'\n",
    "validate_file='validate_data_v1.csv'\n",
    "\n",
    "#We create a function to upload flies to s3 bucket\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "#We upload the files\n",
    "upload_s3_csv(train_file, 'train', train_data)\n",
    "upload_s3_csv(test_file, 'test', test_data)\n",
    "upload_s3_csv(validate_file, 'validate', validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use linear learner estimator to build a classifcation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "#We setup a container\n",
    "container = sagemaker.image_uris.retrieve(\"linear-learner\", region)\n",
    "\n",
    "#We define the classifier\n",
    "classifier1 = Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sagemaker import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "#We define the hyperparameter\n",
    "hyperparameters = {\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"mini_batch_size\": 100,\n",
    "    \"epochs\": 3\n",
    "}\n",
    "\n",
    "\n",
    "#We define the input for the classifier\n",
    "data_train = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_validation = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": data_train,\n",
    "    \"validation\": data_validation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit the model\n",
    "classifier1.set_hyperparameters(**hyperparameters)\n",
    "classifier1.fit(inputs = data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host the model on another instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "#Deploying the model\n",
    "time_stamp = strftime('%d-%H-%M-%S', gmtime())\n",
    "endpoint_name = f'linear-learner-demo-{time_stamp}'\n",
    "print(endpoint_name)\n",
    "predictor = classifier1.deploy(endpoint_name=endpoint_name, initial_instance_count=1, instance_type='ml.m4.xlarge', \n",
    "                               serializer=sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(test_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch transform to evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset without the target column\n",
    "batch_test = testing_dataset.drop(columns=[\"target\"])\n",
    "\n",
    "# Save the modified dataset\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "\n",
    "# Upload the dataset to S3 for batch processing\n",
    "s3_batch_test_path = sess.upload_data(path='batch-in.csv', key_prefix=f'{prefix}/input/testing')\n",
    "print(s3_batch_test_path)\n",
    "\n",
    "# Define the batch output path in S3\n",
    "s3_batch_output_path = f's3://{s3_bucket}/{prefix}/batch-out/'\n",
    "print(s3_batch_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a transformer object from the trained model for batch processing\n",
    "llnearmodel_transform = model.transformer(instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge',\n",
    "                                   strategy='MultiRecord',\n",
    "                                   assemble_with='Line',\n",
    "                                   output_path=s3_batch_output_path)\n",
    "\n",
    "# Start the batch transform job\n",
    "linearmodel_transform.transform(data=s3_batch_test_path,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "# Wait for the batch transform job to complete\n",
    "llnearmodel_transform.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the confusion matrix\n",
    "\n",
    "#We import the libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We create the confusion matrix\n",
    "matrix = confusion_matrix(test_labels, target_predicted_binary)\n",
    "confusion_mat = pd.DataFrame(matrix, index=['Delayed','Not_Delayed'],columns=['Delayed','Not_Delayed'])\n",
    "\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colormap = sns.color_palette(\"BrBG\", 10)\n",
    "sns.heatmap(df_confusion, annot=True, cbar=None, cmap=colormap)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the performance metrics that you see better test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start, extract the values from the confusion matrix cells into variables.\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "\n",
    "print(f\"True Negative (TN) : {TN}\")\n",
    "print(f\"False Positive (FP): {FP}\")\n",
    "print(f\"False Negative (FN): {FN}\")\n",
    "print(f\"True Positive (TP) : {TP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "Sensitivity  = float(TP)/(TP+FN)*100\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")  \n",
    "print(f\"There is a {Sensitivity}% chance of detecting patients with an abnormality have an abnormality\")\n",
    "\n",
    "# Specificity or true negative rate\n",
    "Specificity  = float(TN)/(TN+FP)*100\n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"There is a {Specificity}% chance of detecting normal patients are normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision or positive predictive value\n",
    "Precision = float(TP)/(TP+FP)*100\n",
    "print(f\"Precision: {Precision}%\")  \n",
    "print(f\"You have an abnormality, and the probablity that is correct is {Precision}%\")\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = float(TN)/(TN+FN)*100\n",
    "print(f\"Negative Predictive Value: {NPV}%\") \n",
    "print(f\"You don't have an abnormality, but there is a {NPV}% chance that is incorrect\" )\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = float(FP)/(FP+TN)*100\n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print( f\"There is a {FPR}% chance that this positive result is incorrect.\")\n",
    "\n",
    "# False negative rate\n",
    "FNR = float(FN)/(TP+FN)*100\n",
    "print(f\"False Negative Rate: {FNR}%\") \n",
    "print(f\"There is a {FNR}% chance that this negative result is incorrect.\")\n",
    "\n",
    "# False discovery rate\n",
    "FDR = float(FP)/(TP+FP)*100\n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"You have an abnormality, but there is a {FDR}% chance this is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving a summary\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"Precision: {Precision}%\")   \n",
    "print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print(f\"False Negative Rate: {FNR}%\")  \n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code is for the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the data\n",
    "data = pd.read_csv('combined_csv_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check to see if there are any NA values in the data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do see some negligible amount of NA values, let's drop them\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and testing sets (70% - 15% - 15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the data in train, test and validation accordingly.\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3, random_state= 42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us understand these 3 variables\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We save the data\n",
    "train_data.to_csv(\"train_data_v2.csv\", index = False, header = False)\n",
    "validation_data.to_csv(\"validation_data_v2.csv\", index = False, header = False)\n",
    "test_data_val = test_data.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the sagemaker session and the region\n",
    "sess_sage = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "##We create a default bucket\n",
    "bucket = sess_sage.default_bucket()\n",
    "prefix = \"sagemaker/oncloud\"\n",
    "\n",
    "#We define the flies that are going to be uploaded\n",
    "train_file='train_data_v1.csv'\n",
    "test_file='test_data_v1.csv'\n",
    "validate_file='validate_data_v1.csv'\n",
    "\n",
    "#We create a function to upload flies to s3 bucket\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "#We upload the files\n",
    "upload_s3_csv(train_file, 'train', train_data)\n",
    "upload_s3_csv(test_file, 'test', test_data)\n",
    "upload_s3_csv(validate_file, 'validate', validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use linear learner estimator to build a classifcation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "#We setup a container\n",
    "container = sagemaker.image_uris.retrieve(\"linear-learner\", region)\n",
    "\n",
    "#We define the classifier\n",
    "classifier1 = Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from sagemaker import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "#We define the hyperparameter\n",
    "hyperparameters = {\n",
    "    \"predictor_type\": \"binary_classifier\",\n",
    "    \"mini_batch_size\": 100,\n",
    "    \"epochs\": 3\n",
    "}\n",
    "\n",
    "\n",
    "#We define the input for the classifier\n",
    "data_train = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_validation = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": data_train,\n",
    "    \"validation\": data_validation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit the model\n",
    "classifier1.set_hyperparameters(**hyperparameters)\n",
    "classifier1.fit(inputs = data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host the model on another instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "#Deploying the model\n",
    "time_stamp = strftime('%d-%H-%M-%S', gmtime())\n",
    "endpoint_name = f'linear-learner-demo-{time_stamp}'\n",
    "print(endpoint_name)\n",
    "predictor = classifier1.deploy(endpoint_name=endpoint_name, initial_instance_count=1, instance_type='ml.m4.xlarge', \n",
    "                               serializer=sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(test_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch transform to evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset without the target column\n",
    "batch_test = testing_dataset.drop(columns=[\"target\"])\n",
    "\n",
    "# Save the modified dataset\n",
    "batch_test.to_csv('batch-in.csv', index=False, header=False)\n",
    "\n",
    "# Upload the dataset to S3 for batch processing\n",
    "s3_batch_test_path = sess.upload_data(path='batch-in.csv', key_prefix=f'{prefix}/input/testing')\n",
    "print(s3_batch_test_path)\n",
    "\n",
    "# Define the batch output path in S3\n",
    "s3_batch_output_path = f's3://{s3_bucket}/{prefix}/batch-out/'\n",
    "print(s3_batch_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a transformer object from the trained model for batch processing\n",
    "llnearmodel_transform = model.transformer(instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge',\n",
    "                                   strategy='MultiRecord',\n",
    "                                   assemble_with='Line',\n",
    "                                   output_path=s3_batch_output_path)\n",
    "\n",
    "# Start the batch transform job\n",
    "linearmodel_transform.transform(data=s3_batch_test_path,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "\n",
    "# Wait for the batch transform job to complete\n",
    "llnearmodel_transform.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the confusion matrix\n",
    "\n",
    "#We import the libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We create the confusion matrix\n",
    "matrix = confusion_matrix(test_labels, target_predicted_binary)\n",
    "confusion_mat = pd.DataFrame(matrix, index=['Delayed','Not_Delayed'],columns=['Delayed','Not_Delayed'])\n",
    "\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colormap = sns.color_palette(\"BrBG\", 10)\n",
    "sns.heatmap(df_confusion, annot=True, cbar=None, cmap=colormap)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the performance metrics that you see better test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start, extract the values from the confusion matrix cells into variables.\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "\n",
    "print(f\"True Negative (TN) : {TN}\")\n",
    "print(f\"False Positive (FP): {FP}\")\n",
    "print(f\"False Negative (FN): {FN}\")\n",
    "print(f\"True Positive (TP) : {TP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "Sensitivity  = float(TP)/(TP+FN)*100\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")  \n",
    "print(f\"There is a {Sensitivity}% chance of detecting patients with an abnormality have an abnormality\")\n",
    "\n",
    "# Specificity or true negative rate\n",
    "Specificity  = float(TN)/(TN+FP)*100\n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"There is a {Specificity}% chance of detecting normal patients are normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision or positive predictive value\n",
    "Precision = float(TP)/(TP+FP)*100\n",
    "print(f\"Precision: {Precision}%\")  \n",
    "print(f\"You have an abnormality, and the probablity that is correct is {Precision}%\")\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = float(TN)/(TN+FN)*100\n",
    "print(f\"Negative Predictive Value: {NPV}%\") \n",
    "print(f\"You don't have an abnormality, but there is a {NPV}% chance that is incorrect\" )\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = float(FP)/(FP+TN)*100\n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print( f\"There is a {FPR}% chance that this positive result is incorrect.\")\n",
    "\n",
    "# False negative rate\n",
    "FNR = float(FN)/(TP+FN)*100\n",
    "print(f\"False Negative Rate: {FNR}%\") \n",
    "print(f\"There is a {FNR}% chance that this negative result is incorrect.\")\n",
    "\n",
    "# False discovery rate\n",
    "FDR = float(FP)/(TP+FP)*100\n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"You have an abnormality, but there is a {FDR}% chance this is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving a summary\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"Precision: {Precision}%\")   \n",
    "print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print(f\"False Negative Rate: {FNR}%\")  \n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final comments here and turn the cell type into markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build and evaluate ensemble models\n",
    "\n",
    "Write code to perform the follwoing steps:\n",
    "1. Split data into training, validation and testing sets (70% - 15% - 15%).\n",
    "2. Use xgboost estimator to build a classifcation model.\n",
    "3. Host the model on another instance\n",
    "4. Perform batch transform to evaluate the model on testing data\n",
    "5. Report the performance metrics that you see better test the model performance \n",
    "6. write down your observation on the difference between the performance of using the simple and ensemble models.\n",
    "Note: You are required to perform the above steps on the two combined datasets separatey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the data\n",
    "data = pd.read_csv('combined_csv_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we saw earlier, the data had some NA values in and we have directly removed it.\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and testing sets (70% - 15% - 15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the data in train, test and validation accordingly.\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3, random_state= 42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us understand these 3 variables\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the sagemaker session and the region\n",
    "sess_sage = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "##We create a default bucket\n",
    "bucket = sess_sage.default_bucket()\n",
    "prefix = \"sagemaker/oncloud\"\n",
    "\n",
    "#We define the flies that are going to be uploaded\n",
    "train_file='train_data_v1.csv'\n",
    "test_file='test_data_v1.csv'\n",
    "validate_file='validate_data_v1.csv'\n",
    "\n",
    "#We create a function to upload flies to s3 bucket\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We upload the files\n",
    "upload_s3_csv(train_file, 'train', train_data)\n",
    "upload_s3_csv(test_file, 'test', test_data)\n",
    "upload_s3_csv(validate_file, 'validate', validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use xgboost estimator to build a classifcation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import necessary libraries\n",
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "#We create a container\n",
    "container = get_image_uri(region, 'xgboost', repo_version='1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import necessary libraries\n",
    "import sagemaker\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "\n",
    "#We set an output location\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "\n",
    "#We define our xg boost model\n",
    "xgboost_model=sagemaker.estimator.Estimator(container, \n",
    "                                  role=role,\n",
    "                                  train_instance_count=1,\n",
    "                                  train_instance_type='ml.m4.xlarge',\n",
    "                                  output_path='s3://{}/{}/output'.format(bucket, prefix))\n",
    "\n",
    "xgboost_model.set_hyperparameters(objective='multi:softmax',\n",
    "                                  num_class=2,\n",
    "                                  num_round=10,\n",
    "                                  early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "#We define the input for the classifier\n",
    "data_train = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_validation = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": data_train,\n",
    "    \"validation\": data_validation\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit the model\n",
    "xgboost_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host the model on another instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We deploy the model\n",
    "xgboost_predictor = xgboost_model.deploy(initial_instance_count=1, serializer = sagemaker.serializers.CSVSerializer(), \n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test_data.iloc[0:1,1:]\n",
    "row.head()\n",
    "\n",
    "batch_buffer = io.StringIO()\n",
    "row.to_csv(batch_buffer, header=False, index=False)\n",
    "test_row = batch_buffer.getvalue()\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We predict the test values\n",
    "xgboost_predictor.predict(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the test data again\n",
    "batch_X = test_data.iloc[:,1:];\n",
    "batch_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to delete the end point and this is an important step\n",
    "xgboost_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch transform to evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We conduct batch processing\n",
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)\n",
    "\n",
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgboost_transformer = xgboost_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dowload the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "target_predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_convert(x):\n",
    "    threshold = 0.65\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['binary'] = target_predicted['target'].apply(binary_convert)\n",
    "\n",
    "print(target_predicted.head(10))\n",
    "test.head(10)\n",
    "\n",
    "test_labels = test_data.iloc[:,0]\n",
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the confusion matrix\n",
    "\n",
    "#We import the libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We create the confusion matrix\n",
    "matrix = confusion_matrix(test_labels, target_predicted_binary)\n",
    "confusion_mat = pd.DataFrame(matrix, index=['Delayed','Not_Delayed'],columns=['Delayed','Not_Delayed'])\n",
    "\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#We plot the confusion matrix\n",
    "colormap = sns.color_palette(\"virdis\", 10)\n",
    "sns.heatmap(confusion_mat, annot=True, cbar=None, cmap=colormap)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the performance metrics that you see better test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start, extract the values from the confusion matrix cells into variables.\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "\n",
    "print(f\"True Negative (TN) : {TN}\")\n",
    "print(f\"False Positive (FP): {FP}\")\n",
    "print(f\"False Negative (FN): {FN}\")\n",
    "print(f\"True Positive (TP) : {TP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "Sensitivity  = float(TP)/(TP+FN)*100\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")  \n",
    "print(f\"There is a {Sensitivity}% chance of detecting patients with an abnormality have an abnormality\")\n",
    "\n",
    "# Specificity or true negative rate\n",
    "Specificity  = float(TN)/(TN+FP)*100\n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"There is a {Specificity}% chance of detecting normal patients are normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision or positive predictive value\n",
    "Precision = float(TP)/(TP+FP)*100\n",
    "print(f\"Precision: {Precision}%\")  \n",
    "print(f\"You have an abnormality, and the probablity that is correct is {Precision}%\")\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = float(TN)/(TN+FN)*100\n",
    "print(f\"Negative Predictive Value: {NPV}%\") \n",
    "print(f\"You don't have an abnormality, but there is a {NPV}% chance that is incorrect\" )\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = float(FP)/(FP+TN)*100\n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print( f\"There is a {FPR}% chance that this positive result is incorrect.\")\n",
    "\n",
    "# False negative rate\n",
    "FNR = float(FN)/(TP+FN)*100\n",
    "print(f\"False Negative Rate: {FNR}%\") \n",
    "print(f\"There is a {FNR}% chance that this negative result is incorrect.\")\n",
    "\n",
    "# False discovery rate\n",
    "FDR = float(FP)/(TP+FP)*100\n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"You have an abnormality, but there is a {FDR}% chance this is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving a summary\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"Precision: {Precision}%\")   \n",
    "print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print(f\"False Negative Rate: {FNR}%\")  \n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the data\n",
    "data = pd.read_csv('combined_csv_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we saw earlier, the data had some NA values in and we have directly removed it.\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and testing sets (70% - 15% - 15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split the data in train, test and validation accordingly.\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3, random_state= 42)\n",
    "validation_data, test_data = train_test_split(test_data, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us understand these 3 variables\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the sagemaker session and the region\n",
    "sess_sage = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "##We create a default bucket\n",
    "bucket = sess_sage.default_bucket()\n",
    "prefix = \"sagemaker/oncloud\"\n",
    "\n",
    "#We define the flies that are going to be uploaded\n",
    "train_file='train_data_v2.csv'\n",
    "test_file='test_data_v2.csv'\n",
    "validate_file='validate_data_v2.csv'\n",
    "\n",
    "#We create a function to upload flies to s3 bucket\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit the model\n",
    "xgboost_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host the model on another instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We deploy the model\n",
    "xgboost_predictor = xgboost_model.deploy(initial_instance_count=1, serializer = sagemaker.serializers.CSVSerializer(), \n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test_data.iloc[0:1,1:]\n",
    "row.head()\n",
    "\n",
    "batch_buffer = io.StringIO()\n",
    "row.to_csv(batch_buffer, header=False, index=False)\n",
    "test_row = batch_buffer.getvalue()\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We predict the test values\n",
    "xgboost_predictor.predict(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the test data again\n",
    "batch_X = test_data.iloc[:,1:];\n",
    "batch_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to delete the end point and this is an important step\n",
    "xgboost_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch transform to evaluate the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We conduct batch processing\n",
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)\n",
    "\n",
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgboost_transformer = xgboost_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dowload the results from S3\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "target_predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_convert(x):\n",
    "    threshold = 0.65\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['binary'] = target_predicted['target'].apply(binary_convert)\n",
    "\n",
    "print(target_predicted.head(10))\n",
    "test.head(10)\n",
    "\n",
    "test_labels = test_data.iloc[:,0]\n",
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the confusion matrix\n",
    "\n",
    "#We import the libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We create the confusion matrix\n",
    "matrix = confusion_matrix(test_labels, target_predicted_binary)\n",
    "confusion_mat = pd.DataFrame(matrix, index=['Delayed','Not_Delayed'],columns=['Delayed','Not_Delayed'])\n",
    "\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the performance metrics that you see better test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start, extract the values from the confusion matrix cells into variables.\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "\n",
    "print(f\"True Negative (TN) : {TN}\")\n",
    "print(f\"False Positive (FP): {FP}\")\n",
    "print(f\"False Negative (FN): {FN}\")\n",
    "print(f\"True Positive (TP) : {TP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "Sensitivity  = float(TP)/(TP+FN)*100\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")  \n",
    "print(f\"There is a {Sensitivity}% chance of detecting patients with an abnormality have an abnormality\")\n",
    "\n",
    "# Specificity or true negative rate\n",
    "Specificity  = float(TN)/(TN+FP)*100\n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"There is a {Specificity}% chance of detecting normal patients are normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision or positive predictive value\n",
    "Precision = float(TP)/(TP+FP)*100\n",
    "print(f\"Precision: {Precision}%\")  \n",
    "print(f\"You have an abnormality, and the probablity that is correct is {Precision}%\")\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = float(TN)/(TN+FN)*100\n",
    "print(f\"Negative Predictive Value: {NPV}%\") \n",
    "print(f\"You don't have an abnormality, but there is a {NPV}% chance that is incorrect\" )\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = float(FP)/(FP+TN)*100\n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print( f\"There is a {FPR}% chance that this positive result is incorrect.\")\n",
    "\n",
    "# False negative rate\n",
    "FNR = float(FN)/(TP+FN)*100\n",
    "print(f\"False Negative Rate: {FNR}%\") \n",
    "print(f\"There is a {FNR}% chance that this negative result is incorrect.\")\n",
    "\n",
    "# False discovery rate\n",
    "FDR = float(FP)/(TP+FP)*100\n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"You have an abnormality, but there is a {FDR}% chance this is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving a summary\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"Precision: {Precision}%\")   \n",
    "print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print(f\"False Negative Rate: {FNR}%\")  \n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final comments here and turn the cell type into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
